# -*- coding: utf-8 -*-
"""MAMABA-TKAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13cRQZGli56T3613Tb1mRtPDPzIbmE0h5

# **MAMBA-TKAN-LSTM**
**Wajdi Zarai , Zhaojing Huang (2024)**

## **1. INTRODUCTION**

The development of machine learning, particularly deep learning, has significantly advanced time series
analysis in finance (Filipovi ́c and Khalilzadeh), such as predicting future stock prices using Long and
Short Term Memory (LSTM) networks (Selvin et al.). Building on this, the performance of two
state-of-the-art models for stock price prediction will be examined:
- Kolmogorov-Arnold Networks (KANs) (Liu et al.), especially the Temporal Kolmogorov-Arnold Transformer Networks (TKANs) (Genet and Inzirillo), which utilize an attention mechanism to efficiently capture temporal patterns in multivariate data streams.
- Structured State Space Sequence model (S4) (Gu et al.), with its variant MAMBA (Guand Dao), known for excelling in long-range sequence modeling across domains like vision, language, and audio.

The study will implement these models to predict future stock prices and evaluate their performance within an asset allocation problem.
"""

# Import Librairies

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from google.colab import files
files.upload()

# Improt the stocks data we are working with the indian stock market daily data

import pandas as pd
df = pd.read_csv('BAJAJ-AUTO.csv')[['date', 'close']]
Stocks = ['BAJAJ-AUTO','BAJAJFINSV', 'BAJFINANCE', 'BRITANNIA', 'CIPLA', 'HEROMOTOCO', 'HINDALCO', 'INDUSINDBK', 'ITC', 'KOTAKBANK', 'MARUTI', 'NTPC', 'ONGC', 'RELIANCE', 'SHRIRAMFIN', 'TATACONSUM', 'TATASTEEL', 'TCS', 'TITAN', 'WIPRO', 'NIFTY']
for stock in Stocks:
  df1 = pd.read_csv(stock+'.csv')
  #print(df1.head())
  df[stock]= df1['close']
df.drop(['close'],axis=1, inplace=True)
print(df.head())

df.describe()

df.isna().sum()

df.head(5)

df.tail()



df.dropna(inplace=True)
df.describe()

for stock in Stocks:
  plt.plot(df['date'], df[stock])

Return= df.copy()
Return[Stocks] = Return[Stocks].pct_change()
Return.dropna(inplace=True)
Return

ddate = pd.date_range('6/9/2008', '29/4/2024', freq='D')
ddate

for stock in Stocks:
  plt.plot(Return[stock])

#df.set_index(df.date, inplace=True)
#df.drop('date', axis=1, inplace=True)
df.head()

# we are going in this section to define the output section

df1 = df.copy()
for stock in Stocks:
  df1[stock+ '_output'] = df1[stock].shift(-1)
df1.head()

df_raw = df.copy()
df['NIFTY'] = df['NIFTY'].shift(-1)
df.dropna(inplace=True)
df.tail()



from sklearn.preprocessing import MinMaxScaler

val_split = 0.2
train_split = 0.9
train_size = int(len(df) * train_split)
val_size = int(train_size * val_split)
test_size = int(len(df) - train_size)
window_size = 20

ts = test_size
split_time = len(df) - ts
test_time = df.iloc[split_time + window_size :, 0:1].values

Xdf, ydf = df.iloc[:, 1:21], df.iloc[:, -1]
X = Xdf.astype("float32")
y = ydf.astype("float32")
y_train_set = y[:split_time]
y_test_set = y[split_time:]
X_train_set = X[:split_time]
X_test_set = X[split_time:]
n_features = X_train_set.shape[1]

# Third, we proceed with scaling inputs to the model. Note how this isspecially important now (compare to past tasks) because we are no longer␣dealing with returns, but with prices!
scaler_input = MinMaxScaler(feature_range=(-1, 1))
scaler_input.fit(X_train_set)
X_train_set_scaled = scaler_input.transform(X_train_set)
X_test_set_scaled = scaler_input.transform(X_test_set)
mean_ret = np.mean(y_train_set)
scaler_output = MinMaxScaler(feature_range=(-1, 1))
y_train_set = y_train_set.values.reshape(len(y_train_set), 1)
y_test_set = y_test_set.values.reshape(len(y_test_set), 1)
scaler_output.fit(y_train_set)
y_train_set_scaled = scaler_output.transform(y_train_set)
# Lastly, because we want a time series with up to 20 (window_size) past observations, we need to append these observations into our matrix/vectors!
training_time = df.iloc[:split_time, 0:1].values
X_train = []
y_train = []
for i in range(window_size, y_train_set_scaled.shape[0]):
  X_train.append(X_train_set_scaled[i - window_size : i, :])
  y_train.append(y_train_set_scaled[i])
X_train, y_train = np.array(X_train), np.array(y_train)
print("Size of X vector in training:", X_train.shape)
print("Size of Y vector in training:", y_train.shape)
X_test = []
y_test = y_test_set
for i in range(window_size, y_test_set.shape[0]):
  X_test.append(X_test_set_scaled[i - window_size : i, :])
X_test, y_test = np.array(X_test), np.array(y_test)
print("Size of X vector in test:", X_test.shape)
print("Size of Y vector in test:", y_test.shape)
print("Number of features in the model: ", n_features)

import tensorflow as tf
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], n_features)),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(50, return_sequences=True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(200, return_sequences=True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(200, return_sequences=True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(200, return_sequences=True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(100, return_sequences=False),
  tf.keras.layers.Dense(60),
  tf.keras.layers.Dense(1),
    ])

model.summary()

from keras.callbacks import EarlyStopping
hp_lr = 1e-4
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr), loss="mean_absolute_error")
es = EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=10, restore_best_weights=True)
# fit the models
model.fit(X_train,
  y_train,
  validation_split=0.2,
  epochs=500,
  batch_size=64,
  verbose=1,
  callbacks=[es],
  )

prediction = model.predict(X_test)
print(prediction.shape)

predictions = scaler_output.inverse_transform(prediction)
predictions = predictions.flatten()

values = np.array(y_test[window_size:])
values = values.flatten()
values

df_predictions = pd.DataFrame(
{"Date": test_time.flatten(), "Pred RNN": predictions, "values": values}
)
df_predictions.Date = pd.to_datetime(df_predictions.Date, format="%Y-%m-%d")
df_predictions.head()

import matplotlib.pyplot as plt
plt_1 = plt.figure(figsize=(10, 7))
ax = plt.gca()
df_predictions.plot(x="Date", y="Pred RNN", label="Prediction", ax=ax)
df_predictions.plot(x="Date", y="values", label="Real Price", ax=ax)
plt.grid()
plt.show()

#!pip install mamba-ssm

import torch
from mamba_ssm import Mamba











"""#TKAN"""

#!pip install tkan==0.3.0 tkat==0.1.1
import time
import numpy as np
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, LSTM, Dense, Flatten, Input

from tkan import TKAN
from tkat import TKAT

from sklearn.metrics import r2_score

tf.keras.utils.set_random_seed(1)
tf.config.experimental.enable_op_determinism()

num_unknow_features = len(Stocks)
num_know_features = X_train.shape[2] - num_unknow_features

model_tkan = Sequential([
    Input(shape=X_train.shape[1:]),
    TKAN(100, tkan_activations=[{'grid_size': 3} for i in range(5)], sub_kan_output_dim = 20, sub_kan_input_dim = 1, return_sequences=True),
    TKAN(100, tkan_activations=[{'grid_size': 3} for i in range(5)], sub_kan_output_dim = 20, sub_kan_input_dim = 1, return_sequences=False),
    Dense(units=1, activation='linear')
])


model_tkan.compile(optimizer='adam', loss='mean_squared_error')

model_tkan.summary()

history = model_tkan.fit(X_train, y_train, batch_size=50, epochs=50, validation_split=0.2, verbose = False)

model.summary()

X_test.shape

preds = model.predict(X_test).flatten()

preds.shape

predictions = scaler_output.inverse_transform(preds)
predictions = predictions.flatten()

errors = preds - y_test.flatten()
rmse = np.sqrt(np.mean(np.square(errors)))
r2 = r2_score(y_true=y_test.flatten(), y_pred=preds)
mae = np.mean(np.abs(errors))

metrics_summary = f"""
Model Type: TKAN
------------------------------------
Root Mean Squared Error (RMSE): {rmse:.4f}
R-squared (R²) Score: {r2:.4f}
Mean Absolute Error (MAE): {mae:.4f}
"""
print(metrics_summary)

all_errors = {}
preds = model.predict(X_test)
errors = preds-y_test
all_errors['TKAN'] = errors



"""**References**

- Filipovi ́c, Damir, and Amir Khalilzadeh. “Machine learning for predicting
stock return volatility”.Swiss Finance Institute Research Paper, nos. 21–95, 2021.
- Genet, Remi, and Hugo Inzirillo. “A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting”. arXiv preprint arXiv:2406.02486, 2024.
- Gu, Albert, and Tri Dao. “Mamba: Linear-time sequence modeling with selective state spaces”. arXivpreprint arXiv:2312.00752, 2023.
- Gu, Albert, et al. “Efficiently modeling long sequences with structured state spaces”. arXiv preprintarXiv:2111.00396, 2021.
- Irfan, Mohammad, et al. Advanced Machine Learning Algorithms for Complex Financial Applications.IGI Global, 2023.
- Lim, Bryan, and Stefan Zohren. “Time-series forecasting with deep learning: a survey”. Philosophical Transactions of the Royal Society A, vol. 379, no. 2194, 2021, p. 20200209.
- Liu, Ziming, et al. “Kan: Kolmogorov-arnold networks”. arXiv preprint arXiv:2404.19756, 2024.
- Md Atik Ahameda, Qiang Chenga. “A Time Series is Worth 4 Mambas for Long-term Forecasting”.arXiv preprint arXiv:2403.09898, 2024.
- Selvin, Sreelekshmy, et al. “Stock price prediction using LSTM, RNN and CNN-sliding window model”.2017 international conference on advances in computing, communications and informatics (icacci).IEEE, 2017, pp. 1643–47.
- Shi, Zhuangwei. “Selective state space model for stock prediction”. arXiv preprint arXiv:2402.18959,2024.
- Yuan, Mingsheng, et al. “Is Mamba Effective for Time Series Forecasting?” arXiv preprint arXiv:2403.11144,2024.


"""

